{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Hyperparameter & Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the objective function of Linear SVM:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\bar\\theta) = \\frac{1}{2}\\|\\bar\\theta\\|_2^2 + C\\sum_{i=1}^{N} \\max\\{0, 1 - y^{(i)}\\bar{\\theta}\\cdot \\bar{x}^{(i)}\\}\n",
    "\\end{equation}\n",
    "\n",
    "Given training data, there are two sets of unknowns:\n",
    "- Model parameter(s): the coefficients $\\bar\\theta$, which are found by fitting the model, \n",
    "- Hyperparameter(s): the regularization strength (or its inverse, $C$), which has to be specified prior to the fitting the model.\n",
    "\n",
    "**Note:** Model parameters can be determined through a learning algorithm (e.g., SGD, or calling `fit(X, y)`. But how do we select the best hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download preprocessed data (features + labels). { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf preprocessed\n",
    "!mkdir preprocessed\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/preprocessed/data.npz', 'preprocessed/data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('preprocessed/data.npz') as f:\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    feature_names = f['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to define the preprocessing functions. { display-mode: \"form\" }\n",
    "#@markdown - `impute_missing_values(X)`\n",
    "#@markdown - `normalize_feature_matrix(X)`\n",
    "\n",
    "def impute_missing_values(X):\n",
    "    \"\"\"\n",
    "    For each feature column, impute missing values  (np.nan) with the \n",
    "    population mean for that feature.\n",
    "    \n",
    "    Args:\n",
    "        X: np.array, shape (N, d). X could contain missing values\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). X does not contain any missing values\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer().fit_transform(X)\n",
    "\n",
    "def normalize_feature_matrix(X):\n",
    "    \"\"\"\n",
    "    For each feature column, normalize all values to range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        X: np.array, shape (N, d).\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). Values are normalized per column.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = impute_missing_values(X)\n",
    "X = normalize_feature_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 10 labels:', y[:10])\n",
    "print('First 2 feature vectors:\\n', X[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear') # use default arguments here\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What's wrong with Pipeline v0? How do we fix it? { display-mode: \"form\" }\n",
    "answer =  #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (80%) and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(...)\n",
    "y_pred = clf.predict(...)\n",
    "accuracy = metrics.accuracy_score(...)\n",
    "print('test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What's wrong with Pipeline v1? How do we fix it? { display-mode: \"form\" }\n",
    "answer =  #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in [1e-1, 1, 1e1]:\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print('C={}'.format(C), '\\t', 'test accuracy:', accuracy)\n",
    "# in the end, pick the C with best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What's wrong with Pipeline v2? How do we fix it? { display-mode: \"form\" }\n",
    "answer =  #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline v3: Hyperparameter Selection via Cross validation\n",
    "\n",
    "We will select hyperparameters using 5-fold cross-validation (CV) on the training data. Specifically, we will select the hyperparameters that lead to the best average validation performance across all five folds. The result of hyperparameter selection often depends on the choice of performance measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "First, implement the function `cv_performance(clf, X, y, metric='accuracy', k=5)` to calculate cross-validated performance given a classifier and (training) data `X, y`.\n",
    "\n",
    "When dividing the data into folds for CV, you should try to keep the class proportions (ratio of positive to negative labels) roughly the same across folds. You may employ the following class for splitting the data: `sklearn.model_selection.StratifiedKFold()`. For consistency of results, do not shuffle points when using this function (i.e., do not set `shuffle=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Why might it be beneficial to maintain class proportions across folds? { display-mode: \"form\" }\n",
    "answer = ' ' #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def cv_performance(clf, X, y, metric='accuracy', k=5):\n",
    "    \"\"\"\n",
    "    Splits the training data X and the labels y into k-folds and runs k-fold\n",
    "    cross-validation: for each fold i in 1...k, trains a classifier on\n",
    "    all the data except the ith fold, and tests on the ith fold.\n",
    "    \n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    clf by averaging the performance across folds.\n",
    "    \n",
    "    Input:\n",
    "        clf: a classifier\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "           and d is the number of features\n",
    "        y: (n,) array of binary labels\n",
    "        k: an int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric\n",
    "    \n",
    "    Returns:\n",
    "        average validation performance across the k folds\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    ## HINT: You may find the StratifiedKFold from sklearn.model_selection\n",
    "    ## to be useful\n",
    "\n",
    "    # Put the performance of the model on each fold in the scores array\n",
    "    scores = []\n",
    "\n",
    "    # And return the average performance across all fold splits.\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "Now implement the `select_C_linear_SVM(X, y, metric='accuracy', k=5, C_range=[])` function to choose a value of C for a linear SVM, using 5-fold cross validation on the training data with specified metric. This function should call the `cv_performance` function that you implemented above, passing in instances of `SVC(kernel='linear', C=C)` with a range of values for C chosen in powers of 10 (between $10^{âˆ’3}$ and $10^3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_C_linear_SVM(X, y, metric='accuracy', k=5, C_range=[]):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear SVM, \n",
    "    calculating the k-fold CV performance for each setting of C on \n",
    "    training data X, y.\n",
    "    \n",
    "    Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels\n",
    "        k: int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric\n",
    "        C_range: an array with C values to be searched over\n",
    "    \n",
    "    Returns:\n",
    "        The value of C parameter for a linear SVM that maximizes the\n",
    "        average 5-fold CV performance.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    #HINT: You should be using your cv_performance function here\n",
    "    #to evaluate the performance of each SVM\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "If you need to train a final model, which performance measure would you optimize for when choosing $C$? Explain your choice.\n",
    "\n",
    "Using the training data and functions implemented here, find the best setting for $C$ for your chosen performance measure. Using the best $C$ value you found, train a SVM on the entire training set `X_train, y_train`. Report the performance of this SVM on the test data `X_test, y_test` for each metric below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# Select the best C based on your chosen metric\n",
    "\n",
    "# Train a model using best C on entire training set\n",
    "\n",
    "# Evaluate classifier performance on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric |  Best C | Test Performance |\n",
    "|------|------|------|\n",
    "| accuracy | ? | ? |\n",
    "| F1-score | ? | ? |\n",
    "| AUROC    | ? | ? |\n",
    "| Precision | ? | ? |\n",
    "| Sensitivity | ? | ? |\n",
    "| Specificity | ? | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "The L0-norm of $\\bar{\\theta} \\in \\mathbb{R}^d$ is defined as:\n",
    "\\begin{align*}\n",
    "    \\|\\bar{\\theta}\\|_0 = \\sum_{j=1}^{d} [[\\theta_j \\neq 0]]\n",
    "\\end{align*}\n",
    "where $[[a \\neq 0]]$ is 0 if $a$ is 0 and 1 otherwise.\n",
    "\n",
    "Plot the L0-norm of $\\bar{\\theta}$, the parameter vector learned by the SVM, for each value of $C$. Use the complete training data `X_train, y_train`, i.e, don't use cross-validation for this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "def get_classifier(kernel='linear', penalty='l2', C=1.0, gamma=0.0, class_weight=None):\n",
    "    \"\"\"\n",
    "    Return a linear/rbf kernel SVM classifier based on the given\n",
    "    penalty function and regularization parameter c.\n",
    "    \"\"\"\n",
    "    if penalty == 'l2':\n",
    "        if kernel == 'linear':\n",
    "            return SVC(kernel='linear', C=C, class_weight=class_weight)\n",
    "        elif kernel == 'rbf':\n",
    "            return SVC(kernel='rbf', C=C, gamma=gamma, class_weight=class_weight)\n",
    "    elif penalty == 'l1':\n",
    "        return LinearSVC(penalty='l1', C=C, dual=False, max_iter=20000, class_weight=class_weight)\n",
    "    \n",
    "    raise ValueError('Error: unsupported configuration')\n",
    "\n",
    "def plot_weight(X, y, penalty, C_range):\n",
    "    \"\"\"\n",
    "    Takes as input the training data X and labels y and plots the L0-norm\n",
    "    (number of nonzero elements) of the coefficients learned by a classifier\n",
    "    as a function of the C-values of the classifier.\n",
    "    \"\"\"\n",
    "    print(\"Plotting the number of nonzero entries of the parameter vector as a function of C\")\n",
    "    norm0 = []\n",
    "    \n",
    "    ### Solution\n",
    "    for C in C_range:\n",
    "        clf = get_classifier(kernel='linear', C=C, penalty=penalty)\n",
    "        clf.fit(X, y)\n",
    "        w = clf.coef_\n",
    "        w = np.squeeze(np.asarray(w))\n",
    "        norm0.append(np.linalg.norm((w),ord=0))\n",
    "    ### Solution\n",
    "    \n",
    "    # This code will plot your L0-norm as a function of C\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(C_range, norm0)\n",
    "    plt.xscale('log')\n",
    "    plt.legend(['L0-norm'])\n",
    "    plt.xlabel(\"Value of C\")\n",
    "    plt.ylabel(\"Norm of theta\")\n",
    "    plt.title('Norm-'+penalty+'_penalty.png')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "fig = plot_weight(X_train, y_train, 'l2', np.logspace(-5, 2, 8))\n",
    "plt.ylim(0-1, 40+1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "fig = plot_weight(X_train, y_train, 'l1', np.logspace(-5, 2, 8))\n",
    "plt.ylim(0-1, 40+1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Describe any interesting trends you observe. { display-mode: \"form\" }\n",
    "answer =  #@param {type:\"raw\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
