{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Model Evaluation\n",
    "Reading: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download preprocessed data (features + labels). { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf preprocessed\n",
    "!mkdir preprocessed\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/preprocessed/data.npz', 'preprocessed/data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, calibration, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('preprocessed/data.npz') as f:\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    feature_names = f['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to define the three preprocessing functions. { display-mode: \"form\" }\n",
    "#@markdown - `impute_missing_values(X)`\n",
    "#@markdown - `normalize_feature_matrix(X)`\n",
    "\n",
    "def impute_missing_values(X):\n",
    "    \"\"\"\n",
    "    For each feature column, impute missing values  (np.nan) with the \n",
    "    population mean for that feature.\n",
    "    \n",
    "    Args:\n",
    "        X: np.array, shape (N, d). X could contain missing values\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). X does not contain any missing values\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer().fit_transform(X)\n",
    "\n",
    "def normalize_feature_matrix(X):\n",
    "    \"\"\"\n",
    "    For each feature column, normalize all values to range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        X: np.array, shape (N, d).\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). Values are normalized per column.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = impute_missing_values(X)\n",
    "X = normalize_feature_matrix(X)\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 10 labels:', y[:10])\n",
    "print('First 2 feature vectors:\\n', X[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: training a classifier in python using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "print('Test accuracy:', metrics.accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Quantitative metrics\n",
    "\n",
    "Why isn't accuracy enough? Read about [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), and the related [`sklearn.metrics` submodule](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics). \n",
    "\n",
    "There are many quantitative performance measures that can be derived from the _confusion matrix_:\n",
    "- accuracy\n",
    "- TPR, FPR, TNR, FNR\n",
    "- Precision, recall, sensitivity, specificity\n",
    "- AUROC, AUPR, F1-score\n",
    "- ...\n",
    "\n",
    "Thinking questions:\n",
    "- What score is considered \"good\" for each metric?\n",
    "- Which metric should we optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write a funciton that calculates the following performance measures:\n",
    "#     accuracy, F1-Score, AUROC, precision, sensitivity, and specificity.\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=exceptions.UndefinedMetricWarning)\n",
    "\n",
    "def calculate_performance(clf_trained, X, y_true, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric as evaluated on the true labels\n",
    "    y_true versus the predicted scores from clf_trained and X.\n",
    "    Input:\n",
    "        clf_trained: a fitted instance of sklearn estimator\n",
    "        X : (n,d) np.array containing features\n",
    "        y_true: (n,) np.array containing true labels in {0,1}\n",
    "        metric: string specifying the performance metric; possible options include\n",
    "            'accuracy', 'f1-score', 'auroc', \n",
    "            'precision', 'recall', 'sensitivity', 'specificity', \n",
    "            'tpr', 'fpr', 'tnr', 'fnr'\n",
    "    Returns:\n",
    "        the performance measure as a float\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = ???\n",
    "    if metric.lower() == 'accuracy':\n",
    "        return ???\n",
    "    elif metric.lower() == 'auroc':\n",
    "        return ???\n",
    "    elif metric.lower() == 'f1_score' or metric.lower() == 'f1-score':\n",
    "        return ???\n",
    "    elif ...:\n",
    "        return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(columns=['metric', 'score'])\n",
    "for m in ['accuracy', 'f1-score', 'auroc', 'precision', 'sensitivity', 'specificity']:\n",
    "    df_scores = df_scores.append([\n",
    "        {'metric': m, 'score': calculate_performance(clf, X_test, y_test, metric=m)}\n",
    "    ], ignore_index=True)\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Plots\n",
    "Oftentimes, it is also useful to visually understand a model's predictive power through the following plots:\n",
    "- ROC curve: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "- PR curve: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "- Calibration plot: https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html\n",
    "\n",
    "[Read and discuss.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "y_score = clf.decision_function(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot ROC curve and calculate AUROC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot precision-recall curve and calculate AUPR score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate calibration plot (with deciles) and calculate Brier score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Compare __Linear SVM__ with **Logistic Regression**\n",
    "\n",
    "Using the quantitative metrics and plots we developed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    SVC(kernel='linear', C=1, probability=True).fit(X_train, y_train),\n",
    "    LogisticRegression(C=1).fit(X_train, y_train),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4*) Boostrap confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
