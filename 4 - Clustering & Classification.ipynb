{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Clustering & Classification\n",
    "\n",
    "We have implemented functions that transform the patient data into a feature matrix and label vector, and then split the data to train and test sets, `(X_train, y_train)` and `(X_test, y_test)`. For the tutorial, we will only use 2,000 for training and 500 for test. \n",
    "\n",
    "We will learn a linear classifier to separate the training data into two classes based on whether or not the patient dies during the hospital stay. The labels in `y_train` are binary labels in {âˆ’1, 1}, where 1 means that the patient has died in hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download the data and helper files. { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf data.zip data lib\n",
    "!mkdir lib\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/config.yaml', 'lib/config.yaml')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/helper.py', 'lib/helper.py')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/data.zip', 'data.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lib.helper import load_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell define the three preprocessing functions. { display-mode: \"form\" }\n",
    "def generate_feature_vector(df):\n",
    "    \"\"\"\n",
    "    Reads a dataframe containing all measurements for a single patient\n",
    "    within the first 48 hours of the ICU admission, and convert it into\n",
    "    a feature vector.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.Dataframe, with columns [Time, Variable, Value]\n",
    "    \n",
    "    Returns:\n",
    "        a python dictionary of format {feature_name: feature_value}\n",
    "        for example, {'Age': 32, 'Gender': 0, 'mean_HR': 84, ...}\n",
    "    \"\"\"\n",
    "    static_variables = config['invariant']\n",
    "    timeseries_variables = config['timeseries']\n",
    "\n",
    "    # Replace unknow values\n",
    "    df = df.replace({-1: np.nan})\n",
    "    \n",
    "    # Split time invariant and time series\n",
    "    static, timeseries = df.iloc[0:5], df.iloc[5:]\n",
    "    static = static.pivot('Time', 'Variable', 'Value')\n",
    "\n",
    "    feature_dict = static.iloc[0].to_dict()\n",
    "    for variable in timeseries_variables:\n",
    "        measurements = timeseries[timeseries['Variable'] == variable].Value\n",
    "        feature_dict['mean_' + variable] = np.mean(measurements)\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "def impute_missing_values(X):\n",
    "    \"\"\"\n",
    "    For each feature column, impute missing values  (np.nan) with the \n",
    "    population mean for that feature.\n",
    "    \n",
    "    Args:\n",
    "        X: np.array, shape (N, d). X could contain missing values\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). X does not contain any missing values\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer().fit_transform(X)\n",
    "\n",
    "def normalize_feature_matrix(X):\n",
    "    \"\"\"\n",
    "    For each feature column, normalize all values to range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        X: np.array, shape (N, d).\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). Values are normalized per column.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# `raw_data` is a dictionary mapping patient ID to the data associated with that patient\n",
    "raw_data, df_labels = load_data(N=2500)\n",
    "\n",
    "# Generate features\n",
    "features = [generate_feature_vector(df) for _, df in tqdm(sorted(raw_data.items()), desc='Generating feature vectors')]\n",
    "df_features = pd.DataFrame(features).sort_index(axis=1)\n",
    "feature_names = df_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply imputation and normalization\n",
    "X, y = df_features.values, df_labels['In-hospital_death'].values\n",
    "X = impute_missing_values(X)\n",
    "X = normalize_feature_matrix(X)\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=3)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering & Visualization\n",
    "\n",
    "Clustering should be done **only** using continuous features. \n",
    "\n",
    "- Documentation for [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "- $k$-means Example: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n",
    "- PCA documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- PCA example: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-binary/categorical features\n",
    "X = X_train.copy()\n",
    "X = np.delete(X, [1, 3, 15, 30], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# Perform k-means clustering on X with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# How often does the cluster assignment matches with our label (mortality)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# Run PCA on X\n",
    "\n",
    "\n",
    "# Visualize the resultant clusters in an axis defined by first two principle components of X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas...\n",
    "- Try other values of $k$\n",
    "- Try a different clustering algorithm (e.g., spectral clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training a classifier\n",
    "\n",
    "Implementing ML models using sklearn is very straightforward:\n",
    "```python\n",
    "# 1. Create a classifier\n",
    "clf = sklearn.SomeModel()\n",
    "\n",
    "# 2. Train the classifier\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 3. Use the classifier\n",
    "clf.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a classifier\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train the classifier\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predictions for test set\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = ...\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "Try changing the classifier (use a different class, change the arguments, etc.) and recalculating these scores. Create a table that lists the test accuracy for each model you tried. \n",
    "\n",
    "| Model class |  Test accuracy |\n",
    "|------|------|\n",
    "|   Logistic regression  | ??% |\n",
    "|   SVM  | ??% |\n",
    "|   ...  | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
