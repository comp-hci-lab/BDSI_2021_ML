{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Code: CNNs for sentiment analysis\n",
    "\n",
    "Example of sequence classification using *Convolutional Neural Net* with 1D convolutions.\n",
    "\n",
    "Written by Shengpu Tang (tangsp@umich.edu). The code for dataset and model is \"completely\" original; please feel free to plagiarize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download labeled sentence dataset. { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf sentiment\n",
    "!mkdir -p sentiment\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/sentiment.zip', 'sentiment.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"sentiment.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJa2Dl_LUFqE"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M0PqNXXZW2v"
   },
   "outputs": [],
   "source": [
    "# GPU support\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, path='sentiment/train.txt', labeled=True):\n",
    "        self.labels, self.sentences = [], []\n",
    "        if labeled:\n",
    "            with open(path) as f:\n",
    "                for line in f.readlines():\n",
    "                    self.labels.append(int(line[0]))\n",
    "                    self.sentences.append(line[1:].split())\n",
    "        else:\n",
    "            with open(path) as f:\n",
    "                for line in f.readlines():\n",
    "                    self.labels.append(-1)\n",
    "                    self.sentences.append(line.split())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.labels += other.labels\n",
    "        self.sentences += other.sentences\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr = SentenceDataset('sentiment/train.txt') + SentenceDataset('sentiment/dev.txt')\n",
    "dataset_te = SentenceDataset('sentiment/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_tr[0])\n",
    "print(dataset_tr[7])\n",
    "print(dataset_tr[-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "UNKNOWN = '!!UNKNOWN!!'\n",
    "words = set([UNKNOWN])\n",
    "for sent in dataset_tr.sentences:\n",
    "    words.update(sent)\n",
    "\n",
    "word_to_ix = {w:i for i, w in enumerate(sorted(words))}\n",
    "NUM_LABELS = 2\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idx_vector(sentence, word_to_ix):\n",
    "    vec = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            vec.append(word_to_ix[word])\n",
    "        else:\n",
    "            vec.append(len(word_to_ix) - 1)\n",
    "    return torch.LongTensor(vec)\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, dataset, word_to_idx):\n",
    "        self.dataset = dataset\n",
    "        self.word_to_idx = word_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            make_idx_vector(self.dataset.sentences[idx], self.word_to_idx), \n",
    "            torch.LongTensor([self.dataset.labels[idx]]).float(),\n",
    "        )\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (sentence, label).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuples (sentence, label). \n",
    "            - sentence: torch tensor of shape (?); variable length.\n",
    "            - label: torch tensor of shape ().\n",
    "    Returns:\n",
    "        inputs: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "        labels: torch tensor of shape (batch_size).\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, labels = zip(*data)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(sent) for sent in sentences]\n",
    "    inputs = torch.zeros(len(sentences), max(lengths)).long()\n",
    "    for i, sent in enumerate(sentences):\n",
    "        end = lengths[i]\n",
    "        inputs[i, :end] = sent[:end]        \n",
    "    return inputs, lengths, torch.stack(labels, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding + CNN\n",
    "\n",
    "Embedding layer → 1D-Convolutional Layer → Global Max/Average Pooling → ReLU → Linear → Sigmoid → Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, kernel_size=5, hidden_dim=8, pool='average'):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv = nn.Conv1d(emb_dim, hidden_dim, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.pool = pool\n",
    "    \n",
    "    def forward(self, x, lens):\n",
    "        embed = self.embedding(x).transpose(1,2)\n",
    "        h0 = self.conv(embed)\n",
    "        out = torch.zeros(len(x), self.hidden_dim)\n",
    "        for i, length in enumerate(lens):\n",
    "            if self.pool == 'average':\n",
    "                out[i] = torch.mean(h0[i, :, :lens[i]], dim=1)\n",
    "            elif self.pool == 'max':\n",
    "                out[i], _ = torch.max(h0[i, :, :lens[i]], dim=1)\n",
    "            else:\n",
    "                assert False\n",
    "        h1 = torch.relu(out)\n",
    "        return self.fc(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# model = EmbeddingCNNClassifier(VOCAB_SIZE, 50)\n",
    "model = EmbeddingCNNClassifier(VOCAB_SIZE, 50, kernel_size=7, pool='max').to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "n_epochs = 3\n",
    "tr_losses = []\n",
    "va_losses = []\n",
    "va_scores = []\n",
    "train_counter = []\n",
    "test_counter = [i*len(dataset_tr) for i in range(n_epochs + 1)]\n",
    "\n",
    "tr_loader = DataLoader(EmbeddingDataset(dataset_tr, word_to_ix), batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "va_loader = DataLoader(EmbeddingDataset(dataset_te, word_to_ix), batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Evaluate epoch (before training)\n",
    "y_pred = []\n",
    "val_loss_ = []\n",
    "with torch.no_grad():\n",
    "    for X, lens, y in va_loader:\n",
    "        out = model(X, lens)\n",
    "        loss = criterion(out, y)\n",
    "        val_loss_.append(loss.detach().cpu().item())\n",
    "        y_pred += (out.numpy() > 0).astype(int).tolist()\n",
    "\n",
    "y_pred = np.array(y_pred).ravel()\n",
    "val_score = (y_pred == np.array(dataset_te.labels)).mean()\n",
    "va_scores.append(val_score)\n",
    "va_losses.append(np.mean(val_loss_))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train epoch\n",
    "    for batch_idx, (X, lens, y) in enumerate(tqdm(tr_loader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X, lens)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_losses.append(loss.detach().cpu().item())\n",
    "        train_counter.append((batch_idx*64) + ((epoch)*len(dataset_tr)))\n",
    "    \n",
    "    # Evaluate epoch\n",
    "    y_pred = []\n",
    "    val_loss_ = []\n",
    "    with torch.no_grad():\n",
    "        for X, lens, y in va_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X, lens)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss_.append(loss.detach().cpu().item())\n",
    "            y_pred += (out.numpy() > 0).astype(int).tolist()\n",
    "    \n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    val_score = (y_pred == np.array(dataset_te.labels)).mean()\n",
    "    va_scores.append(val_score)\n",
    "    va_losses.append(np.mean(val_loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(train_counter, tr_losses, ':', color='blue', lw=0.8)\n",
    "plt.scatter(test_counter, va_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('binary cross entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(test_counter, va_scores, color='red')\n",
    "plt.legend(['Validation Accuracy'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.5,1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
