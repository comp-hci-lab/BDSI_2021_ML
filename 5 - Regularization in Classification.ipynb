{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Regularization in Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download the data and helper files. { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf data.zip data lib\n",
    "!mkdir lib\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/config.yaml', 'lib/config.yaml')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/helper.py', 'lib/helper.py')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/data.zip', 'data.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lib.helper import load_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to define the three preprocessing functions. { display-mode: \"form\" }\n",
    "#@markdown - `generate_feature_vector(df)`\n",
    "#@markdown - `impute_missing_values(X)`\n",
    "#@markdown - `normalize_feature_matrix(X)`\n",
    "\n",
    "def generate_feature_vector(df):\n",
    "    \"\"\"\n",
    "    Reads a dataframe containing all measurements for a single patient\n",
    "    within the first 48 hours of the ICU admission, and convert it into\n",
    "    a feature vector.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.Dataframe, with columns [Time, Variable, Value]\n",
    "    \n",
    "    Returns:\n",
    "        a python dictionary of format {feature_name: feature_value}\n",
    "        for example, {'Age': 32, 'Gender': 0, 'mean_HR': 84, ...}\n",
    "    \"\"\"\n",
    "    static_variables = config['invariant']\n",
    "    timeseries_variables = config['timeseries']\n",
    "\n",
    "    # Replace unknow values\n",
    "    df = df.replace({-1: np.nan})\n",
    "    \n",
    "    # Split time invariant and time series\n",
    "    static, timeseries = df.iloc[0:5], df.iloc[5:]\n",
    "    static = static.pivot('Time', 'Variable', 'Value')\n",
    "\n",
    "    feature_dict = static.iloc[0].to_dict()\n",
    "    for variable in timeseries_variables:\n",
    "        measurements = timeseries[timeseries['Variable'] == variable].Value\n",
    "        feature_dict['mean_' + variable] = np.mean(measurements)\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "def impute_missing_values(X):\n",
    "    \"\"\"\n",
    "    For each feature column, impute missing values  (np.nan) with the \n",
    "    population mean for that feature.\n",
    "    \n",
    "    Args:\n",
    "        X: np.array, shape (N, d). X could contain missing values\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). X does not contain any missing values\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer().fit_transform(X)\n",
    "\n",
    "def normalize_feature_matrix(X):\n",
    "    \"\"\"\n",
    "    For each feature column, normalize all values to range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        X: np.array, shape (N, d).\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). Values are normalized per column.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# `raw_data` is a dictionary mapping patient ID to the data associated with that patient\n",
    "raw_data, df_labels = load_data(N=2500)\n",
    "\n",
    "# Generate features\n",
    "features = [generate_feature_vector(df) for _, df in tqdm(sorted(raw_data.items()), desc='Generating feature vectors')]\n",
    "df_features = pd.DataFrame(features).sort_index(axis=1)\n",
    "feature_names = df_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply imputation and normalization\n",
    "X, y = df_features.values, df_labels['In-hospital_death'].values\n",
    "X = impute_missing_values(X)\n",
    "X = normalize_feature_matrix(X)\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=3)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind yourself what the features represent:\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification - Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = [0,1,2,3,6,8,55,88]    # Select an easy subset of patients\n",
    "X = X_train[sel][:, [0,20]]  # Select age and mean_HR\n",
    "y = y_train[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, size=25):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi == -1:\n",
    "            plt.scatter(xi[0], xi[1], c='r', marker='o', s=size)\n",
    "        elif yi == 1:\n",
    "            plt.scatter(xi[0], xi[1], c='g', marker='x', s=size)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('mean_HR')\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_data(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Given these data points, what is your guess for a \"good\" linear classifier? { display-mode: \"form\" }\n",
    "Equation = None #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier with C=1e5\n",
    "clf = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What are the learned model parameters? { display-mode: \"form\" }\n",
    "θ₀ = 0 #@param {type:\"number\"}\n",
    "θ₁ = 0 #@param {type:\"number\"}\n",
    "θ₂ = 0 #@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What is the equation of the classification boundary? { display-mode: \"form\" }\n",
    "Equation = None #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the classification boundary in the same plot. Does it match your intuition?\n",
    "x_clf = np.linspace(0,1,100)\n",
    "y_clf = ???\n",
    "fig = plot_data(X, y)\n",
    "plt.plot(x_clf, y_clf, 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to visualize classification boundary with predicted probabilities. { display-mode: \"form\" }\n",
    "def plot_boundary(X, pred):\n",
    "    try:\n",
    "        x_min, x_max = plt.gca().get_xlim()\n",
    "        y_min, y_max = plt.gca().get_ylim()\n",
    "    except:\n",
    "        x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
    "        y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
    "    xs, ys = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
    "    zs = pred(xys).reshape(xs.shape)\n",
    "    plt.contour(xs, ys, (zs >= 0.5).astype(int), cmap='Greys')\n",
    "    plt.imshow(zs, cmap=\"PiYG\", vmin=-.2, vmax=1.2, alpha=0.4, origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "x_clf = np.linspace(0,1,100)\n",
    "y_clf = (clf.coef_[0,0] * x_clf + clf.intercept_) / -clf.coef_[0,1]\n",
    "fig = plot_data(X, y)\n",
    "plt.plot(x_clf, y_clf, 'k-')\n",
    "plot_boundary(X, lambda xi: clf.predict_proba(xi)[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, train different logistic regression classifiers with C=1e2, C=1e1 and C=1 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to visualize classification boundary with predicted probabilities. { display-mode: \"form\" }\n",
    "def plot_boundary(X, pred):\n",
    "    try:\n",
    "        x_min, x_max = plt.gca().get_xlim()\n",
    "        y_min, y_max = plt.gca().get_ylim()\n",
    "    except:\n",
    "        x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
    "        y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
    "    xs, ys = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
    "    zs = pred(xys).reshape(xs.shape)\n",
    "    plt.contour(xs, ys, (zs >= 0.5).astype(int), cmap='Greys')\n",
    "    plt.imshow(zs, cmap=\"PiYG\", vmin=-.2, vmax=1.2, alpha=0.4, origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "x_clf = np.linspace(0,1,100)\n",
    "y_clf = (clf.coef_[0,0] * x_clf + clf.intercept_) / -clf.coef_[0,1]\n",
    "fig = plot_data(X, y)\n",
    "plt.plot(x_clf, y_clf, 'k-')\n",
    "plot_boundary(X, lambda xi: clf.predict_proba(xi)[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the decision boundaries of LR with different C values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- Try the same experiment with the entire training set\n",
    "- Choose two different features as the input features to your model, or use the first two components of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., use all training data\n",
    "fig = plot_data(X_train[:, [0,20]], y_train, 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
