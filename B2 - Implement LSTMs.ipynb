{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Implement LSTMs\n",
    "\n",
    "**Note:** The only `TODO`s in this exercise is within the `RNN` class. Run the remaining code to load data, train and evaluate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat our task as a sequence classification task and use _Recurrent Neural Networks_. In particular, you will implement an RNN with LSTM cells. Here, we will consider the first 4,000 examples and only consider the 35 time-varying variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download preprocessed data (features + labels). { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf preprocessed\n",
    "!mkdir -p preprocessed\n",
    "!mkdir -p checkpoint\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/preprocessed/data_seq.npz', 'preprocessed/data_seq.npz')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/prepare_data.py', 'lib/prepare_data.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M0PqNXXZW2v"
   },
   "outputs": [],
   "source": [
    "# GPU support\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA7h974JXmVw"
   },
   "outputs": [],
   "source": [
    "import os, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Data preprocessing\n",
    "\n",
    "The features have been preprocessed for you by running `prepare_data.py`. \n",
    "\n",
    "Since the raw data for each patient are irregularly-sampled time-series data, we will first resample them to have even intervals. For the 48-hour period, we group the observations into 2-hour windows so there are 24 time windows. Within each window, we compute the mean value and missing flag for each of the 35 variables. We then use a simple forward imputation approach to estimate some missing values using df.ffill(). This copies an observed value to future unobserved timesteps. All of these preprocessing steps are implemented in generate features(df). The output of this func- tion is a pd.DataFrame with 70 feature columns and 24 rows, one for each 2-hour window. See sample features.csv for an example of the features extracted for one patient.\n",
    "\n",
    "In the `main` function of `prepare_data.py`, we first call `generate_features(df)` on every patient. We then call `impute_missing_values()` to impute the remaining missing values with population mean, and `standardize_features()` to standardize the data to mean 0 and std 1. These functions are very similar to what you did during Week 1. We have already run `prepare_data.py` to preprocess the raw csv files into features and labels, and the output is saved as `data_seq.npz` (provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor([self.y[idx]]).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def get_train_val_test(batch_size=64):\n",
    "    f = np.load('preprocessed/data_seq.npz')\n",
    "    X, y = f['X'], f['y']\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    print('Creating splits')\n",
    "    Xtr, X__, ytr, y__ = train_test_split(X,   y,   train_size=0.8, stratify=y,   random_state=0)\n",
    "    Xva, Xte, yva, yte = train_test_split(X__, y__, test_size=0.5, stratify=y__, random_state=0)\n",
    "    \n",
    "    tr = SimpleDataset(Xtr, ytr)\n",
    "    va = SimpleDataset(Xva, yva)\n",
    "    te = SimpleDataset(Xte, yte)\n",
    "    \n",
    "    tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va, batch_size=batch_size)\n",
    "    te_loader = DataLoader(te, batch_size=batch_size)\n",
    "    \n",
    "    print('Feature shape, Label shape, Class balance:')\n",
    "    print('\\t', tr_loader.dataset.X.shape, tr_loader.dataset.y.shape, tr_loader.dataset.y.mean())\n",
    "    print('\\t', va_loader.dataset.X.shape, va_loader.dataset.y.shape, va_loader.dataset.y.mean())\n",
    "    print('\\t', te_loader.dataset.X.shape, te_loader.dataset.y.shape, te_loader.dataset.y.mean())\n",
    "    return tr_loader, va_loader, te_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loader, va_loader, te_loader = get_train_val_test(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) RNN architecture\n",
    "Study the architecture below. How many learnable parameters are there? Treat biases associated with input and hidden state as different parameters.\n",
    "\n",
    "![](lib/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Implementing an LSTM network\n",
    "\n",
    "Complete the `RNN` model class by implementing the methods `__init()__`, `init_weights()`, `init_hidden(x)` and `forward(x)`. Note that unlike simple feed-forward neural networks, the `forward` function will contain a for-loop since we are dealing with sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aj5H4b2zbjgb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = ???\n",
    "        self.fc = ???\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # TODO: Initialize weights and bias to the specified distributions\n",
    "        ???\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, d = x.shape\n",
    "        \n",
    "        # Initialize hidden states h_0, c_0\n",
    "        h_t, c_t = self.init_hidden(x)\n",
    "        \n",
    "        # Loop through the time-steps\n",
    "        for ???:\n",
    "            # Calculate one time-step\n",
    "            ???\n",
    "        \n",
    "        # Take the output from the final time-step, pass into the fully-connected layer\n",
    "        # Apply output activation\n",
    "        z = ???\n",
    "        return z\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        h_0 = ???\n",
    "        c_0 = ???\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = RNN(70, 64, 1)\n",
    "print('Number of float-valued parameters:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Train the LSTM\n",
    "\n",
    "The data are split into train (80%), validation (10%) and test (10%). Run the following code to train the model and display the resulting training plots (for loss and AUROC). At which epoch do we start overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8Q5n5TWYlC2"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(data_loader, model, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Train the `model` for one epoch of data from `data_loader`\n",
    "    Use `optimizer` to optimize the specified `criterion`\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # clear parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def _evaluate_epoch(tr_loader, va_loader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Evaluate on train\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in tr_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            y_score.append(output.cpu().numpy())\n",
    "            running_loss.append(criterion(output, y).item())\n",
    "\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "        train_loss = np.mean(running_loss)\n",
    "        train_score = metrics.roc_auc_score(y_true, y_score)\n",
    "        print('tr loss', train_loss, 'tr AUROC', train_score)\n",
    "\n",
    "        # Evaluate on validation\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in va_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(X)\n",
    "                y_true.append(y.cpu().numpy())\n",
    "                y_score.append(output.cpu().numpy())\n",
    "                running_loss.append(criterion(output, y).item())\n",
    "\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "        val_loss = np.mean(running_loss)\n",
    "        val_score = metrics.roc_auc_score(y_true, y_score)\n",
    "        print('va loss', val_loss, 'va AUROC', val_score)\n",
    "    return train_loss, val_loss, train_score, val_score\n",
    "\n",
    "def save_checkpoint(model, epoch, checkpoint_dir):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(epoch))\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2238
    },
    "colab_type": "code",
    "id": "1p902gVMaqGt",
    "outputId": "3c2d896d-6efc-453b-a7c5-c8f4dba21c84"
   },
   "outputs": [],
   "source": [
    "tr_loader, va_loader, te_loader = get_train_val_test(batch_size=64)\n",
    "    \n",
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = RNN(70, 64, 1)\n",
    "print('Number of float-valued parameters:', count_parameters(model))\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Epoch', 0)\n",
    "out = _evaluate_epoch(tr_loader, va_loader, model, criterion)\n",
    "outputs.append(out)\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    print('Epoch', epoch+1)\n",
    "    # Train model\n",
    "    _train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "    # Evaluate model\n",
    "    out = _evaluate_epoch(tr_loader, va_loader, model, criterion)\n",
    "    outputs.append(out)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_checkpoint(model, epoch+1, 'checkpoint/')\n",
    "\n",
    "train_losses, val_losses, train_scores, val_scores = zip(*outputs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(range(n_epochs + 1), train_scores, '--o', label='Train')\n",
    "plt.plot(range(n_epochs + 1), val_scores, '--o', label='Validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('AUROC')\n",
    "plt.legend()\n",
    "plt.savefig('auroc.png', dpi=300)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(range(n_epochs + 1), train_losses, '--o', label='Train')\n",
    "plt.plot(range(n_epochs + 1), val_losses, '--o', label='Validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss (binary cross entropy)')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Evaluation on test set\n",
    "\n",
    "Run the following code to evaluate the model on the test set. Choose the epoch number to load the checkpoint from. Report the final model performance on the test set (AUROC score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_checkpoint(model, checkpoint_dir, cuda=False):\n",
    "    \"\"\"\n",
    "    If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
    "    Returns the model and the current epoch.\n",
    "    \"\"\"\n",
    "    cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
    "        if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
    "\n",
    "    if not cp_files:\n",
    "        print('No saved model parameters found')\n",
    "        if force:\n",
    "            raise Exception(\"Checkpoint not found\")\n",
    "        else:\n",
    "            return model, 0, []\n",
    "    \n",
    "    # Find latest epoch\n",
    "    for i in itertools.count(1):\n",
    "        if 'epoch={}.checkpoint.pth.tar'.format(i) in cp_files:\n",
    "            epoch = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Which epoch to load from? Choose in range [1, {}].\".format(epoch))\n",
    "    inp_epoch = int(input())\n",
    "    if inp_epoch not in range(1, epoch+1):\n",
    "        raise Exception(\"Invalid epoch number\")\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir,\n",
    "        'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
    "\n",
    "    print(\"Loading from checkpoint {}\".format(filename))\n",
    "    \n",
    "    if cuda:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(filename,\n",
    "            map_location=lambda storage, loc: storage)\n",
    "\n",
    "    try:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> Successfully restored checkpoint (trained for {} epochs)\"\n",
    "            .format(checkpoint['epoch']))\n",
    "    except:\n",
    "        print(\"=> Checkpoint not successfully restored\")\n",
    "        raise\n",
    "\n",
    "    return model, inp_epoch\n",
    "\n",
    "def _evaluate_epoch(data_loader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            y_true.append(y.numpy())\n",
    "            y_score.append(output)\n",
    "            running_loss.append(criterion(output, y).item())\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "    \n",
    "    loss = np.mean(running_loss)\n",
    "    score = metrics.roc_auc_score(y_true, y_score)\n",
    "    return loss, score\n",
    "\n",
    "_, _, te_loader = get_train_val_test(batch_size=64)\n",
    "model = RNN(70, 64, 1)\n",
    "model, _ = restore_checkpoint(model, 'checkpoint/')\n",
    "criterion = torch.nn.BCELoss()\n",
    "loss, score = _evaluate_epoch(te_loader, model, criterion)\n",
    "print('Test loss :', loss)\n",
    "print('Test AUROC:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Post-mortem / reflection:\n",
    "1. What hyperparameters are associated with this modeling approach? List at least 2.\n",
    "2. Here, we focused on only time-varying data. How might you incorporate the time-invariant data (e.g. age, gender)? Answer is not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
