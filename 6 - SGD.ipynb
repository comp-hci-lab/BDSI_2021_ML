{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to download the data and helper files. { display-mode: \"form\" }\n",
    "!pip install -U wget\n",
    "!rm -rf data.zip data lib\n",
    "!mkdir lib\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/config.yaml', 'lib/config.yaml')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/helper.py', 'lib/helper.py')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/data.zip', 'data.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.helper import load_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to run preprocessing. { display-mode: \"form\" }\n",
    "\n",
    "def generate_feature_vector(df):\n",
    "    \"\"\"\n",
    "    Reads a dataframe containing all measurements for a single patient\n",
    "    within the first 48 hours of the ICU admission, and convert it into\n",
    "    a feature vector.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.Dataframe, with columns [Time, Variable, Value]\n",
    "    \n",
    "    Returns:\n",
    "        a python dictionary of format {feature_name: feature_value}\n",
    "        for example, {'Age': 32, 'Gender': 0, 'mean_HR': 84, ...}\n",
    "    \"\"\"\n",
    "    static_variables = config['invariant']\n",
    "    timeseries_variables = config['timeseries']\n",
    "\n",
    "    # Replace unknow values\n",
    "    df = df.replace({-1: np.nan})\n",
    "    \n",
    "    # Split time invariant and time series\n",
    "    static, timeseries = df.iloc[0:5], df.iloc[5:]\n",
    "    static = static.pivot('Time', 'Variable', 'Value')\n",
    "\n",
    "    feature_dict = static.iloc[0].to_dict()\n",
    "    for variable in timeseries_variables:\n",
    "        measurements = timeseries[timeseries['Variable'] == variable].Value\n",
    "        feature_dict['mean_' + variable] = np.mean(measurements)\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "def impute_missing_values(X):\n",
    "    \"\"\"\n",
    "    For each feature column, impute missing values  (np.nan) with the \n",
    "    population mean for that feature.\n",
    "    \n",
    "    Args:\n",
    "        X: np.array, shape (N, d). X could contain missing values\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). X does not contain any missing values\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer().fit_transform(X)\n",
    "\n",
    "def normalize_feature_matrix(X):\n",
    "    \"\"\"\n",
    "    For each feature column, normalize all values to range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        X: np.array, shape (N, d).\n",
    "    Returns:\n",
    "        X: np.array, shape (N, d). Values are normalized per column.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Load the dataset\n",
    "# `raw_data` is a dictionary mapping patient ID to the data associated with that patient\n",
    "raw_data, df_labels = load_data(N=2500)\n",
    "\n",
    "# Generate features\n",
    "features = [generate_feature_vector(df) for _, df in tqdm(sorted(raw_data.items()), desc='Generating feature vectors')]\n",
    "df_features = pd.DataFrame(features).sort_index(axis=1)\n",
    "feature_names = df_features.columns.tolist()\n",
    "\n",
    "# Apply imputation and normalization\n",
    "X, y = df_features.values, df_labels['In-hospital_death'].values\n",
    "X = impute_missing_values(X)\n",
    "X = normalize_feature_matrix(X)\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=3)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification task, we have feature vectors $\\bar{x} = [x_1, x_2, . . . , x_d]^\\intercal \\in \\mathbb{R}^d$ and labels $y \\in \\{-1,+1\\}$. A classifier is a function mapping from feature vectors to labels $h : \\mathbb{R}^d \\rightarrow \\{-1,+1\\}$. For linear classification, the classifiers are thresholded linear mappings:\n",
    "\n",
    "\\begin{equation*}\n",
    "h(\\bar{x};\\bar\\theta) = \\mathrm{sign}(\\bar\\theta \\cdot \\bar x) = \\Big\\{\\begin{matrix} \n",
    "+1 & \\bar\\theta \\cdot \\bar x > 0\\\\ \n",
    "-1 & \\bar\\theta \\cdot \\bar x \\leq 0\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "We have also defined the following:\n",
    "\n",
    "- Training error\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}_n (\\bar{\\theta}) = \\frac{1}{N} \\sum_{i=1}^N [[y^{(i)} \\neq h(\\bar{x}^{(i)};\\bar{\\theta})]]\n",
    "    \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "- Empirical risk (wrt some loss function)\n",
    "\\begin{equation}\n",
    "    R_n(\\bar{\\theta}) = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{loss}(y \\; (\\bar{\\theta} \\cdot \\bar{x}^{(i)}))\n",
    "    \\nonumber\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen 3 loss functions suitable for a (binary) classification task with label $y\\in\\{-1,+1\\}$:\n",
    "- $\\mathrm{loss}_{0/1}(z) = [[z\\leq 0]]$\n",
    "- $\\mathrm{loss}_{h}(z) = \\max\\{0, 1-z\\}$\n",
    "- $\\mathrm{loss}_{\\log}(z) = \\log_2(1+e^{-z})$\n",
    "\n",
    "Here $z = y \\; (\\bar{\\theta} \\cdot \\bar{x})$; Interpretation for $z$: \"a numerical quantity that indicates whether or not you're right, and to what extent you are right or wrong\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = [0,1,2,3,6,8,55,88]    # Select an easy subset of patients\n",
    "X = X_train[sel][:, [0,20]]  # Select age and mean_HR\n",
    "y = y_train[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, markersize=25):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi == -1:\n",
    "            plt.scatter(xi[0], xi[1], c='r', marker='o', s=markersize)\n",
    "        elif yi == 1:\n",
    "            plt.scatter(xi[0], xi[1], c='g', marker='x', s=markersize)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('mean_HR')\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_data(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) SGD with hinge loss\n",
    "\n",
    "The empirical risk with hinge loss is defined as:\n",
    "\\begin{equation}\n",
    "    J(\\bar{\\theta}) = \\frac{1}{N} \\sum_{i=1}^N \\max\\{0, 1-y \\; (\\bar{\\theta} \\cdot \\bar{x}^{(i)})\\}\n",
    "    \\nonumber\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gradient $\\nabla_{\\bar\\theta}J(\\bar{\\theta})$:\n",
    "\n",
    ">$\\nabla_{\\bar\\theta}J(\\bar{\\theta}) = \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the update rule using SGD upon seeing one example $(\\bar{x}^{(i)}, y^{(i)})$: \n",
    "\n",
    ">$\\bar\\theta^{(k+1)} = \\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement SGD with hinge loss\n",
    "def SGD_hinge(X, y, lr=1e-3, max_iters=1e4):\n",
    "    n, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    losses = []\n",
    "    \n",
    "    n_iter = 0\n",
    "    while n_iter < max_iters:\n",
    "        # Loop through the entire dataset\n",
    "        for xi, yi in zip(X, y):\n",
    "            n_iter += 1\n",
    "            \n",
    "            # Update theta based on each example (xi, yi)\n",
    "            theta = ??\n",
    "            \n",
    "            # Track training progress by calculating empirical risk after each update\n",
    "            losses.append(empirical_risk(X, y, theta))\n",
    "        \n",
    "        # Optionally, shuffle the data points\n",
    "        X, y = ??shuffle(X, y)??\n",
    "    \n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([np.ones((X.shape[0],1)), X], axis=1) # Create a synthetic feature dimension to incorporate the offset term\n",
    "theta, losses = SGD_hinge(X_, y, 1e-2, 1e4)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the classification boundary in the same plot. Does it match your intuition?\n",
    "x_clf = np.linspace(0,1,100)\n",
    "y_clf = (theta_sgd[0] * x_clf + theta_sgd[2]) / -theta_sgd[1]\n",
    "fig = plot_data(X, y)\n",
    "plt.plot(x_clf, y_clf, 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) SGD with hinge loss and L2-regularization\n",
    "\n",
    "The empirical risk with hinge loss and L2-regularization is defined as:\n",
    "\\begin{equation}\n",
    "    J(\\bar{\\theta}) = \\frac{\\lambda}{2}\\|\\bar\\theta\\|_2^2 + \\frac{1}{N} \\sum_{i=1}^N \\max\\{0, 1-y \\; (\\bar{\\theta} \\cdot \\bar{x}^{(i)})\\}\n",
    "    \\nonumber\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gradient $\\nabla_{\\bar\\theta}J(\\bar{\\theta})$:\n",
    "\n",
    ">$\\nabla_{\\bar\\theta}J(\\bar{\\theta}) = \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the update rule using SGD upon seeing one example $(\\bar{x}^{(i)}, y^{(i)})$: \n",
    "\n",
    ">$\\bar\\theta^{(k+1)} = \\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_hinge_L2(X, y, lr=1e-3, lambda_=1):\n",
    "    theta = None\n",
    "    losses = []\n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([X, np.ones((X.shape[0],1))], axis=1)\n",
    "theta, losses = SGD_hinge(X_, y, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Extensions\n",
    "- Experiment with different learning rates and/or regularization strengths.\n",
    "- What would the update rule(s) be if we do not regularize the offset parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Extra: SGD with logistic loss\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\bar\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln(1+\\exp^{-y^{(i)}\\bar{\\theta}\\cdot \\bar{x}^{(i)}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gradient $\\nabla_{\\bar\\theta}J(\\bar{\\theta})$:\n",
    "\n",
    ">$\\nabla_{\\bar\\theta}J(\\bar{\\theta}) = \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the update rule using SGD upon seeing one example $(\\bar{x}^{(i)}, y^{(i)})$: \n",
    "\n",
    ">$\\bar\\theta^{(k+1)} = \\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_logistic(X, y, lr=1e-3):\n",
    "    theta = None\n",
    "    losses = []\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
